{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llms=[LLMConfig(type='openai', model_kwargs={'model_name': 'gpt-4', 'temperature': 0.7}), LLMConfig(type='azure_openai', model_kwargs={'model_name': 'azure-gpt-4', 'temperature': 0.6})] eval_data_set_path='path/to/eval_data.csv' prompt_template_path='path/to/prompt_templates.yaml' read_local_only=True evaluator='custom_evaluator' retriever='custom_retriever'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashwinaravind/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "import yaml\n",
    "import yaml\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any\n",
    "from enum import Enum\n",
    "import importlib\n",
    "\n",
    "# from pydantic import Field, BaseModel, Optional\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "class LLMConfig(BaseModel):\n",
    "    type: str  # LLM type (e.g., \"openai\", \"azure\")\n",
    "    model_kwargs: Dict[str, Any]  # Model-specific arguments\n",
    "\n",
    "class GenerationOptionsConfig(BaseModel):\n",
    "    llms: List[LLMConfig]\n",
    "    eval_data_set_path: str\n",
    "    prompt_template_path: Optional[str] = None \n",
    "    read_local_only: bool\n",
    "    evaluator: str\n",
    "    retriever: Optional[str] = None \n",
    "\n",
    "    @classmethod\n",
    "    def from_yaml(cls, file_path: str) -> \"GenerationOptionsConfig\":\n",
    "        with open(file_path, \"r\") as yaml_file:\n",
    "            config = yaml.safe_load(yaml_file)\n",
    "        return cls(**config[\"Generation\"])\n",
    "    \n",
    "class GenerationConfig(BaseModel):\n",
    "    type: str  # Specifies the LLM type (e.g., \"openai\", \"azure\")\n",
    "    model_kwargs: Optional[Dict[str, Any]] = Field(default_factory=dict, description=\"Model-specific parameters\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    config_path = \"config.yaml\"\n",
    "    generation_options_config = GenerationOptionsConfig.from_yaml(config_path)\n",
    "    print(generation_options_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any\n",
    "from enum import Enum\n",
    "import importlib\n",
    "\n",
    "# Step 1: Lazy Loading Helper Function\n",
    "def lazy_load(module_name: str, class_name: str):\n",
    "    try:\n",
    "        # Dynamically import the module\n",
    "        module = importlib.import_module(module_name)\n",
    "        # Get the class from the module\n",
    "        return getattr(module, class_name)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading {class_name} from module {module_name}: {e}\")\n",
    "\n",
    "# Step 2: Enum Class for LLM Types\n",
    "class LLM(str, Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    AZURE_OPENAI = \"azure_openai\"\n",
    "    HUGGINGFACE = \"huggingface\"\n",
    "    OLLAMA = \"ollama\"\n",
    "    COHERE = \"cohere\"\n",
    "    VERTEXAI = \"vertexai\"\n",
    "    BEDROCK = \"bedrock\"\n",
    "    JINA = \"jina\"\n",
    "    CUSTOM = \"custom\"\n",
    "\n",
    "# Step 3: Map LLM Types to Lazy-loaded Embedding Classes\n",
    "LLM_MAP = {\n",
    "    LLM.OPENAI: lazy_load(\"langchain_openai\", \"ChatOpenAI\"),\n",
    "    LLM.AZURE_OPENAI: lazy_load(\"langchain_openai\", \"AzureChatOpenAI\"),\n",
    "}\n",
    "\n",
    "# Step 4: Define the LLM Configuration Model\n",
    "class LLMConfig(BaseModel):\n",
    "    model_config = {\"protected_namespaces\": ()}\n",
    "    \n",
    "    type: LLM  # Enum to specify the LLM\n",
    "    model_kwargs: Optional[Dict[str, Any]] = Field(default_factory=dict, description=\"Model-specific parameters like model name/type\")\n",
    "    custom_class: Optional[str] = None  # Optional: If using a custom class\n",
    "\n",
    "# Step 5: Load Configuration from YAML\n",
    "def load_config_from_yaml(file_path: str) -> LLMConfig:\n",
    "    print(\"read yaml\")\n",
    "    with open(file_path, \"r\") as file:\n",
    "        config_data = yaml.safe_load(file)\n",
    "    # Convert to LLMConfig object\n",
    "    return LLMConfig(**config_data[\"llm\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "from ragbuilder.generation.config import PromptTemplate\n",
    "import pandas as pd\n",
    "def load_prompts(file_name: str = \"rag_prompts.yaml\", url: str= os.getenv(\"RAG_PROMPT_URL\"),read_local: bool = False):\n",
    "    \"\"\"\n",
    "    Load YAML prompts either from a local file or an online source.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Name of the YAML file. Defaults to \"rag_prompts.yaml\".\n",
    "        read_local (bool): If True, read from a local file. Otherwise, fetch from an online URL.\n",
    "\n",
    "    Returns:\n",
    "        List[PromptTemplate]: A list of PromptTemplate objects.\n",
    "    \"\"\"\n",
    "    yaml_content = None\n",
    "\n",
    "    if read_local:\n",
    "        # Attempt to read from the local file\n",
    "        if os.path.exists(file_name):\n",
    "            print(f\"Loading prompts from local file: {file_name}\")\n",
    "            with open(file_name, 'r') as f:\n",
    "                yaml_content = f.read()\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Local file not found: {file_name}\")\n",
    "    else:\n",
    "        # Attempt to fetch from an online source\n",
    "        print(f\"Fetching prompts from online file: {url}\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an HTTP error for bad responses\n",
    "            yaml_content = response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise RuntimeError(f\"Failed to load prompts from URL {url}: {e}\")\n",
    "\n",
    "    # Parse the YAML content\n",
    "    try:\n",
    "        prompts_data = yaml.safe_load(yaml_content)\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Failed to parse YAML content: {e}\")\n",
    "\n",
    "    # Convert YAML entries into PromptTemplate objects\n",
    "    prompts = [\n",
    "        PromptTemplate(name=entry['name'], template=entry['template'])\n",
    "        for entry in prompts_data\n",
    "    ]\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Type\n",
    "\n",
    "class SystemPromptGenerator:\n",
    "    def __init__(self, config: \"GenerationOptionsConfig\", evaluator_class: Type):\n",
    "        \"\"\"\n",
    "        Initialize the SystemPromptGenerator with generation options and evaluator class.\n",
    "\n",
    "        Args:\n",
    "            config (GenerationOptionsConfig): Configuration object containing options for generation.\n",
    "            evaluator_class (Type): Evaluator class to be instantiated for evaluation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.evaluator = evaluator_class()\n",
    "        self.prompt_templates = load_prompts(config.prompt_template_path)\n",
    "        print(self.config)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _load_prompt_templates(file_path: str) -> list:\n",
    "    #     \"\"\"\n",
    "    #     Load prompt templates from a YAML file.\n",
    "\n",
    "    #     Args:\n",
    "    #         file_path (str): Path to the YAML file containing prompt templates.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list: A list of loaded prompt templates.\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         with open(file_path, \"r\") as yaml_file:\n",
    "    #             templates = yaml.safe_load(yaml_file)\n",
    "    #         return templates.get(\"prompts\", [])\n",
    "    #     except Exception as e:\n",
    "    #         raise ValueError(f\"Failed to load prompt templates: {e}\")\n",
    "\n",
    "    def _build_trial_config(self, options_config: \"GenerationOptionsConfig\") -> List[GenerationConfig]:\n",
    "        \"\"\"\n",
    "        Build a list of GenerationConfig objects from the provided GenerationOptionsConfig.\n",
    "\n",
    "        Args:\n",
    "            options_config (GenerationOptionsConfig): The input configuration for trial generation.\n",
    "\n",
    "        Returns:\n",
    "            List[GenerationConfig]: A list of generated configurations for trials.\n",
    "        \"\"\"\n",
    "        trial_configs = []\n",
    "        for llm_config in options_config.llms:\n",
    "            llm_instance = LLMConfig(type=llm_config.type, model_kwargs=llm_config.model_kwargs)\n",
    "            print(llm_instance)\n",
    "            trial_config = GenerationConfig(\n",
    "                type=llm_instance,  # Pass the LLMConfig instance here\n",
    "                model_kwargs=llm_config.model_kwargs,\n",
    "                evaluator=options_config.evaluator,\n",
    "                retriever=options_config.retriever,\n",
    "                eval_data_set_path=options_config.eval_data_set_path,\n",
    "                prompt_template_path=options_config.prompt_template_path,\n",
    "                read_local_only=options_config.read_local_only,)\n",
    "            trial_configs.append(trial_config)\n",
    "        trial_configs = []\n",
    "        # for llm_config in options_config.llms:\n",
    "        #     print(llm_config)\n",
    "        #     trial_config = GenerationConfig(\n",
    "        #         llm_type=llm_config.type,\n",
    "        #         llm_model_kwargs=llm_config.model_kwargs,\n",
    "        #         evaluator=options_config.evaluator,\n",
    "        #         retriever=options_config.retriever,\n",
    "        #         eval_data_set_path=options_config.eval_data_set_path,\n",
    "        #         prompt_template_path=options_config.prompt_template_path,\n",
    "        #         read_local_only=options_config.read_local_only,\n",
    "        #     )\n",
    "        #     trial_configs.append(trial_config)\n",
    "        return trial_configs\n",
    "        return None\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Optimize the prompt templates by running the pipeline for each generation configuration\n",
    "        and evaluating the results.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the best prompt template and its score.\n",
    "        \"\"\"\n",
    "        print(\"Optimization started...\")\n",
    "\n",
    "        # Generate all trial configurations.\n",
    "        trial_configs = self._build_trial_config(self.config)\n",
    "        return \n",
    "        best_prompt = None\n",
    "        best_score = float(\"-inf\")\n",
    "        for trial_config in trial_configs:\n",
    "            print(f\"Running pipeline for LLM: {trial_config.llm_model_kwargs['model']}...\")\n",
    "\n",
    "            # Call the pipeline function with the trial configuration.\n",
    "            pipeline_results = self._run_pipeline(trial_config)\n",
    "            print(f\"Pipeline results: {pipeline_results}\")\n",
    "            # Evaluate the results using the evaluator.\n",
    "            # evaluated_results = self.evaluator.evaluate(\n",
    "            #     pipeline_results,\n",
    "            #     llm=trial_config.llm_model_kwargs,\n",
    "            #     embeddings=trial_config.llm_model_kwargs,  # Adjust this based on embedding logic.\n",
    "            # )\n",
    "\n",
    "            # # Compute the average score for this trial.\n",
    "            # average_score = self._calculate_average_score(evaluated_results)\n",
    "            # print(f\"Average Score for {trial_config.llm_model_kwargs['model']}: {average_score}\")\n",
    "\n",
    "            # # Update the best prompt template if this trial is better.\n",
    "            # if average_score > best_score:\n",
    "            #     best_score = average_score\n",
    "            #     best_prompt = trial_config\n",
    "\n",
    "        print(f\"Optimization completed. Best Score: {best_score}\")\n",
    "        return {\n",
    "            \"best_prompt\": 'best_prompt',\n",
    "            \"best_score\": 'best_score',\n",
    "        }\n",
    "\n",
    "    def _run_pipeline(self, trial_config: GenerationConfig) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Placeholder function to run the RAG pipeline for the given trial configuration.\n",
    "\n",
    "        Args:\n",
    "            trial_config (GenerationConfig): The configuration to use for the pipeline.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: Results from running the pipeline.\n",
    "        \"\"\"\n",
    "        # Replace with your actual pipeline invocation logic.\n",
    "        # For now, this is a placeholder.\n",
    "        return [\n",
    "            {\"question\": \"What is AI?\", \"answer\": \"Artificial Intelligence\", \"context\": \"AI is ...\"}\n",
    "        ]\n",
    "\n",
    "    def _calculate_average_score(self, evaluated_results: List[Dict]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the average score from evaluated results.\n",
    "\n",
    "        Args:\n",
    "            evaluated_results (List[Dict]): The results from the evaluator.\n",
    "\n",
    "        Returns:\n",
    "            float: The average score across all results.\n",
    "        \"\"\"\n",
    "        scores = [result.get(\"score\", 0) for result in evaluated_results]\n",
    "        return sum(scores) / len(scores) if scores else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from ragbuilder.generation.config import EvalDataset\n",
    "from ragbuilder.generation.utils import get_eval_dataset\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness\n",
    ")\n",
    "from ragas import evaluate, RunConfig\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "class Evaluator(ABC):\n",
    "    @abstractmethod\n",
    "    def evaluate(self, eval_dataset: Dataset) -> Dataset:\n",
    "        \"\"\"\n",
    "        Evaluate the prompt generation Phase and returns detailed results.\n",
    "        \n",
    "        Returns:\n",
    "        Dataset: A dataset containing the evaluation results.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.eval_dataset = None\n",
    "class RAGASEvaluator(Evaluator):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        print(\"RAGASEvaluator initiated\")\n",
    "    \n",
    "    def evaluate(self, eval_dataset: Dataset,llm= AzureChatOpenAI(model=\"gpt-4o-mini\"), embeddings=AzureOpenAIEmbeddings(model=\"text-embedding-3-large\"))-> Dataset:\n",
    "        result = evaluate(\n",
    "                eval_dataset,\n",
    "                metrics=[\n",
    "                    answer_correctness,\n",
    "                    faithfulness,\n",
    "                    answer_relevancy,\n",
    "                    context_precision,\n",
    "                    context_recall,\n",
    "                ],\n",
    "                raise_exceptions=False, \n",
    "                is_async=True,\n",
    "                run_config=RunConfig(timeout=240, max_workers=1, max_wait=180, max_retries=10)\n",
    "            )\n",
    "        result_df = result.to_pandas()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_csv_path = 'rag_eval_results_'+timestamp+'.csv'\n",
    "        selected_columns = [\"prompt_key\",\"prompt\",\"question\",\"answer\",\"ground_truth\",\"answer_correctness\",\"faithfulness\",\"answer_relevancy\",\"context_precision\",\"context_recall\"]\n",
    "        result_df[selected_columns].to_csv(output_csv_path, index=False)\n",
    "        print(\"evaluate_prompts completed\")\n",
    "        print(Dataset.from_pandas(result_df[selected_columns]))\n",
    "        return Dataset.from_pandas(result_df[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_optimization_core(options_config: GenerationOptionsConfig):\n",
    "    \"\"\"\n",
    "    Core function to perform RAG optimization based on the given options config.\n",
    "\n",
    "    Args:\n",
    "        options_config (GenerationOptionsConfig): Configuration for RAG optimization.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Best configuration, best score, and evaluation results.\n",
    "    \"\"\"\n",
    "    # Load environment variables first\n",
    "    # load_environment()\n",
    "    # missing_vars = validate_environment(options_config)\n",
    "\n",
    "    # if missing_vars:\n",
    "    #     raise ValueError(\n",
    "    #         \"Missing required environment variables for selected components:\\n\" +\n",
    "    #         \"\\n\".join(f\"- {var}\" for var in missing_vars)\n",
    "    #     )\n",
    "\n",
    "    # Graph handling, if applicable\n",
    "    # if options_config.graph:\n",
    "    #     print(\"Loading graph...\")\n",
    "    #     config = GenerationConfig(\n",
    "    #         llm_type=options_config.graph.llm.type,\n",
    "    #         llm_model_kwargs=options_config.graph.llm.model_kwargs,\n",
    "    #         prompt_template_path=options_config.prompt_template_path,\n",
    "    #         retriever=options_config.graph.retriever,\n",
    "    #     )\n",
    "    #     pipeline = RAGPipeline(config)\n",
    "    #     graph_chunks = pipeline.chunk_and_ingest()\n",
    "        \n",
    "    #     # Ensure the graph can operate with the LLM\n",
    "    #     llm = options_config.graph.llm\n",
    "    #     load_graph(graph_chunks, llm)\n",
    "\n",
    "    # Create evaluator\n",
    "    # if options_config.evaluator.type == EvaluatorType.CUSTOM:\n",
    "    #     # module_path, class_name = options_config.evaluator.custom_class.rsplit('.', 1)\n",
    "    #     module = import_module(module_path)\n",
    "    #     evaluator_class = getattr(module, class_name)\n",
    "    #     evaluator = evaluator_class(options_config.eval_data_set_path, options_config.evaluator)\n",
    "    # else:\n",
    "    #     evaluator = RAGASEvaluator(\n",
    "    #         options_config.eval_data_set_path, options_config.evaluator\n",
    "    #     )\n",
    "    evaluator = RAGASEvaluator\n",
    "\n",
    "    # Run optimization\n",
    "    optimizer = SystemPromptGenerator(options_config, evaluator)\n",
    "    optimizer.optimize()\n",
    "    return\n",
    "    best_config, best_score = optimizer.optimize()\n",
    "\n",
    "\n",
    "    # # Create and run pipeline with best configuration for caching\n",
    "    # pipeline = RAGPipeline(best_config)\n",
    "    # best_index = pipeline.run()\n",
    "\n",
    "    # # Store best configuration key in DocumentStore or equivalent\n",
    "    # # optimizer.doc_store.set_best_config_key(pipeline.loader_key, pipeline.config_key)\n",
    "\n",
    "    # console.print(\"[success]âœ“ Successfully optimized and cached best configuration[/success]\")\n",
    "    # return best_config, best_score, best_index\n",
    "\n",
    "def run_optimization(options_config_path: str):\n",
    "    \"\"\"\n",
    "    Run RAG optimization using configuration from a YAML file.\n",
    "\n",
    "    Args:\n",
    "        options_config_path (str): Path to the configuration YAML file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Best configuration, best score, and evaluation results.\n",
    "    \"\"\"\n",
    "    options_config = GenerationOptionsConfig.from_yaml(options_config_path)\n",
    "    return _run_optimization_core(options_config)\n",
    "\n",
    "def run_optimization_from_dict(options_config_dict: dict):\n",
    "    \"\"\"\n",
    "    Run RAG optimization using configuration provided as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        options_config_dict (dict): Configuration as a dictionary.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: Best configuration, best score, and evaluation results.\n",
    "    \"\"\"\n",
    "    options_config = GenerationOptionsConfig(**options_config_dict)\n",
    "    return _run_optimization_core(options_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGASEvaluator initiated\n",
      "Fetching prompts from online file: https://raw.githubusercontent.com/ashwinaravind/rag_prompts/refs/heads/main/rag_prompts.yml\n",
      "llms=[LLMConfig(type='openai', model_kwargs={'model_name': 'gpt-4', 'temperature': 0.7}), LLMConfig(type='azure_openai', model_kwargs={'model_name': 'azure-gpt-4', 'temperature': 0.6})] eval_data_set_path='path/to/eval_data.csv' prompt_template_path='path/to/prompt_templates.yaml' read_local_only=True evaluator='custom_evaluator' retriever='custom_retriever'\n",
      "Optimization started...\n",
      "type=<LLM.OPENAI: 'openai'> model_kwargs={'model_name': 'gpt-4', 'temperature': 0.7} custom_class=None\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GenerationConfig\ntype\n  Input should be a valid string [type=string_type, input_value=LLMConfig(type=<LLM.OPENA...0.7}, custom_class=None), input_type=LLMConfig]\n    For further information visit https://errors.pydantic.dev/2.8/v/string_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[114], line 77\u001b[0m, in \u001b[0;36mrun_optimization\u001b[0;34m(options_config_path)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mRun RAG optimization using configuration from a YAML file.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    Tuple: Best configuration, best score, and evaluation results.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m options_config \u001b[38;5;241m=\u001b[39m GenerationOptionsConfig\u001b[38;5;241m.\u001b[39mfrom_yaml(options_config_path)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_optimization_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[114], line 51\u001b[0m, in \u001b[0;36m_run_optimization_core\u001b[0;34m(options_config)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[1;32m     50\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m SystemPromptGenerator(options_config, evaluator)\n\u001b[0;32m---> 51\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     53\u001b[0m best_config, best_score \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39moptimize()\n",
      "Cell \u001b[0;32mIn[112], line 85\u001b[0m, in \u001b[0;36mSystemPromptGenerator.optimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimization started...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Generate all trial configurations.\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m trial_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_trial_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \n\u001b[1;32m     87\u001b[0m best_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[112], line 49\u001b[0m, in \u001b[0;36mSystemPromptGenerator._build_trial_config\u001b[0;34m(self, options_config)\u001b[0m\n\u001b[1;32m     47\u001b[0m     llm_instance \u001b[38;5;241m=\u001b[39m LLMConfig(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mllm_config\u001b[38;5;241m.\u001b[39mtype, model_kwargs\u001b[38;5;241m=\u001b[39mllm_config\u001b[38;5;241m.\u001b[39mmodel_kwargs)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(llm_instance)\n\u001b[0;32m---> 49\u001b[0m     trial_config \u001b[38;5;241m=\u001b[39m \u001b[43mGenerationConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the LLMConfig instance here\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_data_set_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_data_set_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_template_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_template_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_local_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_local_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     trial_configs\u001b[38;5;241m.\u001b[39mappend(trial_config)\n\u001b[1;32m     58\u001b[0m trial_configs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/pydantic/main.py:192\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    191\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for GenerationConfig\ntype\n  Input should be a valid string [type=string_type, input_value=LLMConfig(type=<LLM.OPENA...0.7}, custom_class=None), input_type=LLMConfig]\n    For further information visit https://errors.pydantic.dev/2.8/v/string_type"
     ]
    }
   ],
   "source": [
    "run_optimization('config.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
